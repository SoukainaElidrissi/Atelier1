{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1575,
          "sourceType": "datasetVersion",
          "datasetId": 854
        }
      ],
      "dockerImageVersionId": 30664,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "AtelierEx1",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoukainaElidrissi/Atelier1/blob/main/AtelierEx1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'nyse:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F854%2F1575%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240310%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240310T144226Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D647fe2f02e5b79b7e86222eb7871952645bf573796d2949877082b3d5032829081743ab9ed58bf144d9f196a927070de2166af1d3e03c538d8c352786484007393b0bf15e45eea6b0037db495410be7a118dad3c3537d06520ed1d7c1e3d980b591eabae74db5da9dea3d5fb8ec6127088775c5dda6ea464350cd7b8e0d8a69963c0efa4c5a7062ce4f7ccac8e10a27bded8f78cf6f3c291c0d2ec960c208a7e00d4e811dbb44cf7dc8f9b6586f39b3d5ecd7997ccf24d32091192c07b1c22ed6a1db796f644176746524ed90718cf239e614a89c6894f9c2632cfe708641d148ee66d85ed8963bda0676dc5fc74e0bc64a03bd786cbc03e030acafb660a2230'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "5NbgUIrD0I6-"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-03-10T14:42:18.490049Z"
        },
        "trusted": true,
        "id": "GFiqXIeM0I7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 1: Exploratory Data Analysis (EDA)\n",
        "#Prices-split-adjusted\n",
        "df = pd.read_csv(\"/kaggle/input/nyse/prices-split-adjusted.csv\", index_col = 0)\n",
        "df.info()\n",
        "df.head()\n",
        "df.dropna(inplace=True)\n",
        "df.isnull().sum()\n",
        "df.describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Tn5WKnN50I7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "symbols_to_plot = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']\n",
        "plt.figure(figsize=(10, 6))\n",
        "for symbol in symbols_to_plot:\n",
        "    plt.plot(df[df['symbol'] == symbol].index, df[df['symbol'] == symbol]['close'], label=symbol)\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Closing Prices of Select Symbols over Time')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "oWNFGSoj0I7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df[df['symbol'] == 'AAPL'].index, df[df['symbol'] == 'AAPL']['volume'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volume')\n",
        "plt.title('Trading Volume of AAPL over Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "jjB8AsPd0I7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "QHK-okwd0I7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "4k9PWqEL0I7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(['close'], axis=1)\n",
        "y = df['close']"
      ],
      "metadata": {
        "trusted": true,
        "id": "wc87B8wz0I7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prices\n",
        "prices_dataframe = pd.read_csv('/kaggle/input/nyse/prices.csv')\n",
        "prices_dataframe.info()\n",
        "prices_dataframe.head()\n",
        "prices_dataframe.dropna(inplace=True)\n",
        "prices_dataframe.isnull().sum()\n",
        "prices_dataframe.describe()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZNeIeiT00I7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prices_dataframe.columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tR7_KrQc0I7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices_dataframe = prices_dataframe.loc[prices_dataframe['symbol']=='AAPL']\n",
        "prices_dataframe.drop('symbol', axis=1, inplace=True)\n",
        "print(np.shape(prices_dataframe))\n",
        "prices_dataframe.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "W6NOEJ0A0I7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing the index to date\n",
        "\n",
        "prices_dataframe['date'] = pd.to_datetime(prices_dataframe['date'])\n",
        "prices_dataframe = prices_dataframe.set_index('date')\n",
        "\n",
        "prices_dataframe.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "WHapf9FO0I7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting individual numerical columns over time\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.subplot(3, 2, 1)\n",
        "plt.plot(prices_dataframe.index, prices_dataframe['open'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Open Price')\n",
        "plt.title('Open Price over Time')\n",
        "\n",
        "plt.subplot(3, 2, 2)\n",
        "plt.plot(prices_dataframe.index, prices_dataframe['close'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.title('Close Price over Time')\n",
        "\n",
        "plt.subplot(3, 2, 3)\n",
        "plt.plot(prices_dataframe.index, prices_dataframe['low'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Low Price')\n",
        "plt.title('Low Price over Time')\n",
        "\n",
        "plt.subplot(3, 2, 4)\n",
        "plt.plot(prices_dataframe.index, prices_dataframe['high'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('High Price')\n",
        "plt.title('High Price over Time')\n",
        "\n",
        "plt.subplot(3, 2, 5)\n",
        "plt.plot(prices_dataframe.index, prices_dataframe['volume'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Volume')\n",
        "plt.title('Volume over Time')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "Yqt9_ES60I7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prices_dataframe = prices_dataframe.sample(frac=0.75, replace=False, random_state=7)\n",
        "prices_dataframe['year'] = prices_dataframe.index.year\n",
        "prices_dataframe['month'] = prices_dataframe.index.month\n",
        "prices_dataframe['day'] = prices_dataframe.index.day"
      ],
      "metadata": {
        "trusted": true,
        "id": "nrZ2C6Y-0I7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fundamentals\n",
        "fundamentals_dataframe = pd.read_csv('/kaggle/input/nyse/fundamentals.csv', index_col=['Unnamed: 0'])\n",
        "fundamentals_dataframe.info()\n",
        "fundamentals_dataframe.head()\n",
        "fundamentals_dataframe.dropna(inplace=True)\n",
        "fundamentals_dataframe.isnull().sum()\n",
        "fundamentals_dataframe.describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "xc7wxBPo0I7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fundamentals_dataframe.isnull().sum(axis=0)\n",
        "\n",
        "missing_columns = ['Current Ratio', 'Cash Ratio', 'Quick Ratio', 'For Year', 'Earnings Per Share', 'Estimated Shares Outstanding']\n",
        "for column in missing_columns:\n",
        "    median_value = fundamentals_dataframe[column].median()\n",
        "    fundamentals_dataframe[column] = fundamentals_dataframe[column].fillna(median_value)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ml8IHYGm0I7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Securities\n",
        "securities_dataframe = pd.read_csv('/kaggle/input/nyse/securities.csv')\n",
        "securities_dataframe.info()\n",
        "securities_dataframe.head()\n",
        "securities_dataframe.dropna(inplace=True)\n",
        "securities_dataframe.isnull().sum()\n",
        "securities_dataframe.describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "YKkFG4ov0I7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "securities_dataframe=securities_dataframe.dropna(subset=['Date first added'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "h8G9vI9b0I7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "securities_dataframe.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "hAxst1SZ0I7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import vstack,sqrt\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.utils.data import Dataset,DataLoader,random_split\n",
        "from torch import Tensor\n",
        "from torch.nn import ReLU,Module,MSELoss,Linear\n",
        "from torch.optim import SGD\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from tqdm import tqdm\n",
        "\n",
        "# dataset definition perparation\n",
        "class CSVDataset(Dataset):\n",
        "    # load the dataset\n",
        "    def __init__(self, path):\n",
        "        # load the csv file as a dataframe\n",
        "        df = read_csv(path, header=None)\n",
        "\n",
        "        df.head()\n",
        "        # store the inputs and outputs\n",
        "        self.X = df.values[:, :-1].astype('float32')\n",
        "        self.y = df.values[:, -1].astype('float32')\n",
        "        # ensure target has the right shape\n",
        "        self.y = self.y.reshape((len(self.y), 1))\n",
        "\n",
        "    # number of rows in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    # get a row at an index\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        "\n",
        "    # get indexes for train and test rows\n",
        "    def get_splits(self, n_test=0.33):\n",
        "        # determine sizes\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        train_size = len(self.X) - test_size\n",
        "        # calculate the split\n",
        "        return random_split(self, [train_size, test_size])\n",
        "\n",
        "# model definition\n",
        "class MLP(Module):\n",
        "    # define model elements\n",
        "    def __init__(self, n_inputs):\n",
        "        super(MLP, self).__init__()\n",
        "        # input to first hidden layer\n",
        "        self.hidden1 = Linear(n_inputs, 10)\n",
        "        xavier_uniform_(self.hidden1.weight)\n",
        "        self.act1 = ReLU()\n",
        "        # second hidden layer\n",
        "        self.hidden2 = Linear(10, 8)\n",
        "        xavier_uniform_(self.hidden2.weight)\n",
        "        self.act2 = ReLU()\n",
        "        # third hidden layer and output\n",
        "        self.hidden3 = Linear(8, 1)\n",
        "        xavier_uniform_(self.hidden3.weight)\n",
        "\n",
        "    # forward propagate input\n",
        "    def forward(self, X):\n",
        "        # input to first hidden layer\n",
        "        X = self.hidden1(X)\n",
        "        X = self.act1(X)\n",
        "        # second hidden layer\n",
        "        X = self.hidden2(X)\n",
        "        X = self.act2(X)\n",
        "        # third hidden layer and output\n",
        "        X = self.hidden3(X)\n",
        "        return X\n",
        "\n",
        "# prepare the dataset\n",
        "def prepare_data(path):\n",
        "    # load the dataset\n",
        "    dataset = CSVDataset(path)\n",
        "    # calculate split\n",
        "    train, test = dataset.get_splits()\n",
        "    # prepare data loaders\n",
        "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "    test_dl = DataLoader(test, batch_size=32, shuffle=False)\n",
        "    return train_dl, test_dl\n",
        "\n",
        "# train the model\n",
        "def train_model(train_dl, model):\n",
        "    size = len(train_dl.dataset)\n",
        "    # define the optimization\n",
        "    criterion = MSELoss()\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    # enumerate epochs\n",
        "    # enumerate epochs\n",
        "    for epoch in tqdm(range(100),desc='Training Epochs'):\n",
        "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "        # enumerate mini batches\n",
        "        for batch, (inputs, targets) in enumerate(train_dl):\n",
        "            # clear the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # compute the model output\n",
        "            yhat = model(inputs)\n",
        "            # calculate loss\n",
        "            loss = criterion(yhat, targets)\n",
        "            # credit assignment\n",
        "            loss.backward()\n",
        "            # update model weights\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                loss, current = loss.item(), batch * len(inputs)\n",
        "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# evaluate the model\n",
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        # evaluate the model on the test set\n",
        "        yhat = model(inputs)\n",
        "        # retrieve numpy array\n",
        "        yhat = yhat.detach().numpy()\n",
        "        actual = targets.numpy()\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        # store\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    # calculate mse\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    return mse\n",
        "# make a class prediction for one row of data\n",
        "def predict(row, model):\n",
        "    # convert row to data\n",
        "    row = Tensor([row])\n",
        "    # make prediction\n",
        "    yhat = model(row)\n",
        "    # retrieve numpy array\n",
        "    yhat = yhat.detach().numpy()\n",
        "    return yhat"
      ],
      "metadata": {
        "trusted": true,
        "id": "5IUGr9Rz0I7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/kaggle/input/nyse/prices-split-adjusted.csv\")\n",
        "df = pd.DataFrame(data)\n",
        "X = df.drop(['date', 'symbol', 'volume'], axis=1)\n",
        "X = X[['low', 'high', 'open', 'close']]\n",
        "\n",
        "csv_path = '/kaggle/working/MyWork.csv'\n",
        "csv_path1 = '/kaggle/working/MyWork1.csv'\n",
        "\n",
        "X.to_csv(csv_path, index=False)\n",
        "X = pd.read_csv(csv_path, skiprows=1)\n",
        "X.to_csv(csv_path1, index=False)\n",
        "\n",
        "train_dl, test_dl = prepare_data(csv_path1)\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "model = MLP(3)\n",
        "train_losses = train_model(train_dl, model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UQfNzHTp0I7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}